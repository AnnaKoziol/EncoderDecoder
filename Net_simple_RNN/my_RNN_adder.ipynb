{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Avg. Loss for last 500 samples = inf\n",
      "Final total loss is:tensor(0.2328)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "random.seed(10)\n",
    "\n",
    "def getSample(stringLength, testFlag):\n",
    "  lowerBound=pow(2,stringLength-1)+1\n",
    "  upperBound=pow(2,stringLength)\n",
    "\n",
    "  num1=random.randint(lowerBound,upperBound)\n",
    "  num2=random.randint(lowerBound,upperBound)\n",
    "\n",
    "  num3=num1+num2\n",
    "  num3Binary=(bin(num3)[2:])\n",
    "  num1Binary=(bin(num1)[2:])\n",
    "  num2Binary=(bin(num2)[2:])\n",
    "\n",
    "  if testFlag==1:\n",
    "    print('input numbers and their sum  are', num1, ' ', num2, ' ', num3)\n",
    "    print ('binary strings are', num1Binary, ' ' , num2Binary, ' ' , num3Binary)\n",
    "  len_num3= (len(num3Binary))\n",
    "\n",
    "  # since num3 will be the largest, we pad other numbers with zeros to that num3_len\n",
    "  num1Binary= ('0'*(len(num3Binary)-len(num1Binary))+num1Binary)\n",
    "  num2Binary= ('0'*(len(num3Binary)-len(num2Binary))+num2Binary)\n",
    "\n",
    "  # forming the input sequence\n",
    "  # the input at first timestep is the least significant bits of the two input binary strings\n",
    "  # x will be then a len_num3 ( or T ) * 2 array\n",
    "  x=np.zeros((len_num3,2),dtype=np.float32)\n",
    "  for i in range(0, len_num3):\n",
    "    x[i,0]=num1Binary[len_num3-1-i] # note that MSB of the binary string should be the last input along the time axis\n",
    "    x[i,1]=num2Binary[len_num3-1-i]\n",
    "  y = np.fromiter(num3Binary[::-1], dtype=np.int)\n",
    "  return x,y\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self, inputDim, hiddenDim, outputDim):\n",
    "    super(Model, self).__init__()\n",
    "    self.inputDim = inputDim\n",
    "    self.hiddenDim = hiddenDim\n",
    "    self.outputDim = outputDim\n",
    "    self.rnn = nn.LSTM(inputDim, hiddenDim)\n",
    "    self.outputLayer = nn.Linear(hiddenDim, outputDim)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "  def forward(self, x):\n",
    "    #size of x is T x B x featDim\n",
    "    #B = 1 is dummy batch dimension added, because pytorch mandates it\n",
    "    #if you want B as first dimension of x then specify batchFirst=True when LSTM is initalized\n",
    "    #T,D  = x.size(0), x.size(1)\n",
    "    #batch is a must\n",
    "    out,hidden = self.rnn(x) #x has two  dimensions  seqLen *batch* FeatDim=2\n",
    "    T,B,D  = out.size(0), out.size(1), out.size(2)\n",
    "    out = out.contiguous()\n",
    "    out = out.view(B*T, D)\n",
    "    outputLayerActivations = self.outputLayer(out)\n",
    "    outputSigmoid = self.sigmoid(outputLayerActivations)\n",
    "    return outputSigmoid\n",
    "\n",
    "featDim = 2 # two bits each from each of the String\n",
    "outputDim = 1 # one output node which would output a zero or 1\n",
    "\n",
    "rnnSize=10\n",
    "\n",
    "lossFunction = nn.MSELoss()\n",
    "model = Model(featDim, rnnSize, outputDim)\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "epochs=500\n",
    "totalLoss= float(\"inf\")\n",
    "\n",
    "print(\" Avg. Loss for last 500 samples = %lf\"%(totalLoss))\n",
    "totalLoss=0\n",
    "for i in range(0,epochs): # average the loss over 200 samples\n",
    "    stringLen=4\n",
    "    testFlag=0\n",
    "    x,y = getSample(stringLen, testFlag)\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    x_var=autograd.Variable(torch.from_numpy(x).unsqueeze(1).float()) #convert to torch tensor and variable\n",
    "    # unsqueeze() is used to add the extra dimension since\n",
    "    # your input need to be of t*batchsize*featDim; you cant do away with the batch in pytorch\n",
    "    seqLen = x_var.size(0)\n",
    "    x_var = x_var.contiguous()\n",
    "    y_var = autograd.Variable(torch.from_numpy(y).float())\n",
    "    finalScores = model(x_var)\n",
    "\n",
    "    loss = lossFunction(finalScores,y_var)\n",
    "    totalLoss+=loss.data\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "totalLoss = totalLoss/epochs\n",
    "print('Final total loss is:' + str(totalLoss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input numbers and their sum  are 83   93   176\n",
      "binary strings are 1010011   1011101   10110000\n",
      "sum predicted by RNN is  [ True  True  True  True  True  True  True  True]\n",
      "##################################################\n",
      "input numbers and their sum  are 100   93   193\n",
      "binary strings are 1100100   1011101   11000001\n",
      "sum predicted by RNN is  [ True  True  True  True  True  True  True  True]\n",
      "##################################################\n",
      "input numbers and their sum  are 79   72   151\n",
      "binary strings are 1001111   1001000   10010111\n",
      "sum predicted by RNN is  [ True  True  True  True  True  True  True  True]\n",
      "##################################################\n",
      "input numbers and their sum  are 72   118   190\n",
      "binary strings are 1001000   1110110   10111110\n",
      "sum predicted by RNN is  [ True  True  True  True  True  True  True  True]\n",
      "##################################################\n",
      "input numbers and their sum  are 116   69   185\n",
      "binary strings are 1110100   1000101   10111001\n",
      "sum predicted by RNN is  [ True  True  True  True  True  True  True  True]\n",
      "##################################################\n",
      "input numbers and their sum  are 77   81   158\n",
      "binary strings are 1001101   1010001   10011110\n",
      "sum predicted by RNN is  [ True  True  True  True  True  True  True  True]\n",
      "##################################################\n",
      "input numbers and their sum  are 119   104   223\n",
      "binary strings are 1110111   1101000   11011111\n",
      "sum predicted by RNN is  [ True  True  True  True  True  True  True  True]\n",
      "##################################################\n",
      "input numbers and their sum  are 71   93   164\n",
      "binary strings are 1000111   1011101   10100100\n",
      "sum predicted by RNN is  [ True  True  True  True  True  True  True  True]\n",
      "##################################################\n",
      "input numbers and their sum  are 119   105   224\n",
      "binary strings are 1110111   1101001   11100000\n",
      "sum predicted by RNN is  [ True  True  True  True  True  True  True  True]\n",
      "##################################################\n",
      "input numbers and their sum  are 101   121   222\n",
      "binary strings are 1100101   1111001   11011110\n",
      "sum predicted by RNN is  [ True  True  True  True  True  True  True  True]\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "###### Testing the model ######\n",
    "\n",
    "stringLen=7\n",
    "testFlag=1\n",
    "# test the network on 10 random binary string addition cases where stringLen=4\n",
    "for i in range (0,10):\n",
    "\tx,y=getSample(stringLen,testFlag)\n",
    "\tx_var=autograd.Variable(torch.from_numpy(x).unsqueeze(1).float())\n",
    "\ty_var=autograd.Variable(torch.from_numpy(y).float())\n",
    "\tseqLen=x_var.size(0)\n",
    "\tx_var= x_var.contiguous()\n",
    "\tfinalScores = model(x_var).data.t()\n",
    "\t#print(finalScores)\n",
    "\tbits=finalScores.gt(0.5)\n",
    "\tbits=bits[0].numpy()\n",
    "\n",
    "\tprint ('sum predicted by RNN is ',bits[::-1])\n",
    "\tprint('##################################################')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}