{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized\n",
      "Final total loss is:tensor(0.2468)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AGH\\Projects\\ANN\\CNN_autoencoder\\project\\env\\lib\\site-packages\\torch\\nn\\modules\\loss.py:446: UserWarning: Using a target size (torch.Size([9])) that is different to the input size (torch.Size([9, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/mineshmathew/pyTorch_RNN_Examples/blob/master/BinaryStringAddition/AddBinaryStrings.py.ipynb\n",
    "# ============================================================================\n",
    "# Make a simple RNN learn binary addition\n",
    "# ============================================================================\n",
    "# author  mineshmathew.github.io\n",
    "# ============================================================================\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import random\n",
    "import sys\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.random.manual_seed(10)\n",
    "\n",
    "def getSample(stringLength, testFlag):\n",
    "  #takes stringlength as input\n",
    "  #returns a sample for the network - an input sequence - x and its target -y\n",
    "  #x is a T*2 array, T is the length of the string and 2 since we take one bit each from each string\n",
    "  #testFlag if set prints the input numbers and its sum in both decimal and binary form\n",
    "  lowerBound=pow(2,stringLength-1)+1\n",
    "  upperBound=pow(2,stringLength)\n",
    "\n",
    "  num1=random.randint(lowerBound,upperBound)\n",
    "  num2=random.randint(lowerBound,upperBound)\n",
    "\n",
    "  num3=num1+num2\n",
    "  num3Binary=(bin(num3)[2:])\n",
    "  num1Binary=(bin(num1)[2:])\n",
    "  num2Binary=(bin(num2)[2:])\n",
    "\n",
    "  if testFlag==1:\n",
    "    print('input numbers and their sum are: ', num1, ', ', num2, ', ', num3)\n",
    "    print ('binary strings are: ', num1Binary, ', ' , num2Binary, ', ' , num3Binary)\n",
    "  len_num1= (len(num1Binary))\n",
    "  len_num2= (len(num2Binary))\n",
    "  len_num3= (len(num3Binary))\n",
    "\n",
    "  # since num3 will be the largest, we pad other numbers with zeros to that num3_len\n",
    "  num1Binary= ('0'*(len(num3Binary)-len(num1Binary))+num1Binary)\n",
    "  num2Binary= ('0'*(len(num3Binary)-len(num2Binary))+num2Binary)\n",
    "\n",
    "  #num1Binary = np.array(list(reversed(num1Binary)))\n",
    "  #num2Binary = np.array(list(reversed(num2Binary)))\n",
    "  #num3Binary = np.array(list(reversed(num3Binary)))\n",
    "\n",
    "  # forming the input sequence\n",
    "  # the input at first timestep is the least significant bits of the two input binary strings\n",
    "  # x will be then a len_num3 ( or T ) * 2 array\n",
    "  x=np.zeros((len_num3,2),dtype=np.int)\n",
    "  for i in range(0, len_num3):\n",
    "    x[i,0]=num1Binary[len_num3-1-i] # note that MSB of the binary string should be the last input along the time axis\n",
    "    x[i,1]=num2Binary[len_num3-1-i]\n",
    "  # target vector is the sum in binary\n",
    "  # convert binary string in <string> to a numpy 1D array\n",
    "  #https://stackoverflow.com/questions/29091869/convert-bitstring-string-of-1-and-0s-to-numpy-array\n",
    "  # print('num3Binary')\n",
    "  #y=np.array(map(int, num3Binary[::-1]))\n",
    "  y = np.fromiter(num3Binary[::-1], dtype=np.int)\n",
    "  # print('x and y are')\n",
    "  # print (x)\n",
    "  # print (y)\n",
    "  if testFlag==1:\n",
    "    print('a,b,c current  are: {},{},{}'.format(np.array(x[:,0]),\n",
    "                                              np.array(x[:,1]),\n",
    "                                              np.array(y)))\n",
    "  return x,y\n",
    "\n",
    "class Adder (nn.Module):\n",
    "  def __init__(self, inputDim, hiddenDim, outputDim):\n",
    "    super(Adder, self).__init__()\n",
    "    self.inputDim=inputDim\n",
    "    self.hiddenDim=hiddenDim\n",
    "    self.outputDim=outputDim\n",
    "    self.rnn=nn.RNN(inputDim, hiddenDim)\n",
    "    self.outputLayer=nn.Linear(hiddenDim, outputDim)\n",
    "    self.sigmoid=nn.Sigmoid()\n",
    "  def forward(self, x):\n",
    "    #size of x is T x B x featDim\n",
    "    #B=1 is dummy batch dimension added, because pytorch mandates it\n",
    "    #if you want B as first dimension of x then specift batchFirst=True when LSTM is initalized\n",
    "    #T,D  = x.size(0), x.size(1)\n",
    "    #batch is a must\n",
    "    rnnOut,_ = self.rnn(x) #x has two  dimensions  seqLen *batch* FeatDim=2\n",
    "    T,B,D  = rnnOut.size(0),rnnOut.size(1),rnnOut.size(2)\n",
    "    rnnOut = rnnOut.contiguous()\n",
    "        # before  feeding to linear layer we squash one dimension\n",
    "    rnnOut = rnnOut.view(B*T, D)\n",
    "    outputLayerActivations=self.outputLayer(rnnOut)\n",
    "    #reshape activations to T*B*outputlayersize\n",
    "    outputLayerActivations=outputLayerActivations.view(T,B,-1).squeeze(1)\n",
    "    outputSigmoid=self.sigmoid(outputLayerActivations)\n",
    "    return outputSigmoid\n",
    "\n",
    "featDim=2 #two bits each from each of the String\n",
    "outputDim=1 #one output node which would output a zero or 1\n",
    "lstmSize=16\n",
    "\n",
    "lossFunction = nn.MSELoss()\n",
    "model = Adder(featDim, lstmSize, outputDim)\n",
    "print ('Model initialized')\n",
    "#optimizer = optim.SGD(model.parameters(), lr=3e-2, momentum=0.8)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "#optimizer=optim.Adam(model.parameters(), lr=0.1)\n",
    "epochs=15000\n",
    "### epochs ##\n",
    "#totalLoss= float(\"inf\")\n",
    "#print(\"Avg. Loss for last 500 samples = %lf\"%(totalLoss))\n",
    "totalLoss=0\n",
    "for i in range(0,epochs): # average the loss over 200 samples\n",
    "    stringLen=8\n",
    "    testFlag=0\n",
    "    x,y=getSample(stringLen, testFlag)\n",
    "\n",
    "    model.zero_grad()\n",
    "\n",
    "    x_var=autograd.Variable(torch.from_numpy(x).unsqueeze(1).float()) #convert to torch tensor and variable\n",
    "    # unsqueeze() is used to add the extra dimension since\n",
    "    # your input need to be of t*batchsize*featDim; you cant do away with the batch in pytorch\n",
    "    seqLen=x_var.size(0)\n",
    "    x_var= x_var.contiguous()\n",
    "    y_var=autograd.Variable(torch.from_numpy(y).float())\n",
    "    finalScores = model(x_var)\n",
    "    #finalScores=finalScores.\n",
    "\n",
    "    loss=lossFunction(finalScores,y_var)\n",
    "    totalLoss+=loss.data\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "totalLoss=totalLoss/epochs\n",
    "print('Final total loss is:' + str(totalLoss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input numbers and their sum are:  29 ,  17 ,  46\n",
      "binary strings are:  11101 ,  10001 ,  101110\n",
      "a,b,c current  are: [1 0 1 1 1 0],[1 0 0 0 1 0],[0 1 1 1 0 1]\n",
      "x and y are: [[1 1]\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [0 0]], [0 1 1 1 0 1]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "t() expects a tensor with <= 2 dimensions, but self is 3D",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-985051e4bccf>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     11\u001B[0m         \u001B[0mseqLen\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mx_var\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m         \u001B[0mx_var\u001B[0m\u001B[1;33m=\u001B[0m \u001B[0mx_var\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcontiguous\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m         \u001B[0mfinalScores\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx_var\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mt\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'model output: {}'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfinalScores\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m         \u001B[0mbits\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mround\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfinalScores\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: t() expects a tensor with <= 2 dimensions, but self is 3D"
     ]
    }
   ],
   "source": [
    "###### Testing the model ######\n",
    "\n",
    "stringLen=5\n",
    "testFlag=1\n",
    "# test the network on 10 random binary string addition cases where stringLen=4\n",
    "for i in range (0,10):\n",
    "\tx,y=getSample(stringLen,testFlag)\n",
    "\tprint('x and y are: {}, {}'.format(x,y))\n",
    "\tx_var=autograd.Variable(torch.from_numpy(x).unsqueeze(1).float())\n",
    "\ty_var=autograd.Variable(torch.from_numpy(y).float())\n",
    "\tseqLen=x_var.size(0)\n",
    "\tx_var= x_var.contiguous()\n",
    "\tfinalScores = model(x_var).data.t()\n",
    "\tprint('model output: {}'.format(finalScores))\n",
    "\tbits=np.round(finalScores)\n",
    "\tprint('result is {}'.format(bits))\n",
    "\n",
    "\t#print('sum predicted by RNN is ',bits[::-1])\n",
    "\tprint('--------------------------------------------')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}